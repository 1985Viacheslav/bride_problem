{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оптимизация выбора лучшего жениха методом обучения с подкреплением (Reinforcement Learning)\n",
    "\n",
    "Задача:\n",
    "1. Невеста ищет себе жениха (существует единственное вакантное место).\n",
    "2. Есть известное число претендентов — N.\n",
    "3. Невеста общается с претендентами в случайном порядке, с каждым не более одного раза.\n",
    "4. О каждом претенденте известно, лучше он или хуже любого из предыдущих.\n",
    "5. Пообщавшись с претендентом, невеста сравнивает его с предыдущими и либо отказывает, либо принимает его предложение. Если предложение принято, они женятся и процесс останавливается. Если невеста отказывает жениху, то вернуться к нему позже она не сможет.\n",
    "6. Невеста выигрывает, если она выберет самого лучшего претендента. Выбор даже второго по порядку сравнения — проигрыш.\n",
    "\n",
    "Требуется найти решение, с наибольшей вероятностью приводящее к выбору самого лучшего претендента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формулировка задачи:\n",
    "\n",
    "Невеста должна выбрать лучшего жениха среди N претендентов, с каждым из которых она общается только один раз.\n",
    "\n",
    "Она должна либо принять, либо отклонить жениха на месте.\n",
    "\n",
    "Она выигрывает, если выберет самого лучшего претендента.\n",
    "\n",
    "## Импорт необходимых библиотек\n",
    "\n",
    "Для решения задачи будем использовать следующие библиотеки: gym для создания среды, stable_baselines3 для алгоритмов усиленного обучения, numpy для работы с массивами данных и json для сохранения результатов.\n",
    "\n",
    "# Создание среды:\n",
    "\n",
    "Определим среду для нашей задачи. Среда будет содержать список женихов с случайными рейтингами. Невеста будет принимать решение о каждом женихе по очереди, основываясь на стратегии, обученной с использованием методов обучения с подкреплением.\n",
    "\n",
    "Мы создаем симуляционную среду BrideEnv с помощью библиотеки gym.\n",
    "\n",
    "В этой среде претенденты появляются в случайном порядке с разными рейтингами.\n",
    "\n",
    "Невеста может принять или отклонить каждого претендента.\n",
    "\n",
    "Определение действий и наблюдений:\n",
    "\n",
    "Действия: 0 - отклонить претендента, 1 - принять претендента.\n",
    "\n",
    "Наблюдения: рейтинг текущего претендента.\n",
    "\n",
    "## Метод reset\n",
    "\n",
    "Метод `reset` подготавливает новую случайную последовательность женихов, устанавливает начальные значения переменных и возвращает рейтинг первого жениха для принятия решения.\n",
    "\n",
    "## Метод step\n",
    "\n",
    "Метод `step` реализует логику принятия решения агентом. Если агент принимает жениха, проверяется, является ли он лучшим. В случае отказа переходим к следующему жениху.\n",
    "\n",
    "# Модель обучения:\n",
    "\n",
    "Мы используем алгоритм PPO из библиотеки stable_baselines3 для обучения модели.\n",
    "\n",
    "Модель обучается на множественных итерациях, чтобы оптимизировать стратегию выбора жениха.\n",
    "\n",
    "# Обучение модели:\n",
    "\n",
    "Модель повторяет процесс выбора жениха много раз, чтобы найти оптимальную стратегию.\n",
    "\n",
    "В каждом эпизоде модель принимает решение для каждого претендента, и обучается на основе полученной награды (выбор лучшего претендента).\n",
    "\n",
    "# Процесс выбора лучшего жениха\n",
    "Процесс выбора лучшего жениха с использованием обученной модели. Агент принимает решение о выборе или отказе от каждого жениха и обновляет состояние среды.\n",
    "\n",
    "# Оценка и проверка:\n",
    "\n",
    "После обучения, мы проверяем, как модель принимает решения в новых сценариях.\n",
    "\n",
    "Сохраняем данные об итерациях и шагах в JSON-файл для дальнейшего анализа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.96     |\n",
      "|    ep_rew_mean     | 0.08     |\n",
      "| time/              |          |\n",
      "|    fps             | 2464     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.3          |\n",
      "|    ep_rew_mean          | 0.13         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1231         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0110673765 |\n",
      "|    clip_fraction        | 0.0756       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.685       |\n",
      "|    explained_variance   | -0.58        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0124      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0236      |\n",
      "|    value_loss           | 0.0763       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.53        |\n",
      "|    ep_rew_mean          | 0.14        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1063        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041523546 |\n",
      "|    clip_fraction        | 0.507       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.618      |\n",
      "|    explained_variance   | 0.0845      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0479     |\n",
      "|    value_loss           | 0.096       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.83        |\n",
      "|    ep_rew_mean          | 0.25        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1000        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069776885 |\n",
      "|    clip_fraction        | 0.466       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.47       |\n",
      "|    explained_variance   | 0.0999      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00786     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0476     |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 3.03       |\n",
      "|    ep_rew_mean          | 0.39       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 965        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05457115 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.343     |\n",
      "|    explained_variance   | 0.0947     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0328     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0317    |\n",
      "|    value_loss           | 0.159      |\n",
      "----------------------------------------\n",
      "Обученная модель выбрала лучшего жениха с рейтингом 10 после 1 итераций\n",
      "Данные успешно сохранены в iteration_log.json\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "import json\n",
    "\n",
    "class BrideEnv(gym.Env):\n",
    "    def __init__(self, num_grooms=10):\n",
    "        super(BrideEnv, self).__init__()\n",
    "        self.num_grooms = num_grooms  # Количество женихов\n",
    "        self.action_space = spaces.Discrete(2)  # Пространство действий: 0 - Отказать, 1 - Принять\n",
    "        self.observation_space = spaces.Box(low=1, high=num_grooms, shape=(1,), dtype=np.int32)  # Пространство наблюдений\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.grooms = np.random.permutation(self.num_grooms) + 1  # Перемешиваем женихов и назначаем им рейтинги\n",
    "        self.current_index = 0  # Начинаем с первого жениха\n",
    "        self.done = False  # Устанавливаем, что процесс выбора не завершен\n",
    "        self.max_groom = max(self.grooms)  # Находим жениха с максимальным рейтингом\n",
    "        self.selected_groom = -1  # Изначально жених не выбран\n",
    "        return np.array([self.grooms[self.current_index]])  # Возвращаем обсервацию\n",
    "    \n",
    "    def step(self, action):\n",
    "        try:\n",
    "            if action == 1:  # Если агент решил принять жениха\n",
    "                self.selected_groom = self.grooms[self.current_index]  # Сохраняем выбранного жениха\n",
    "                reward = 1 if self.selected_groom == self.max_groom else 0  # Награда за выбор лучшего жениха\n",
    "                self.done = True  # Завершаем процесс выбора\n",
    "                return np.array([self.selected_groom]), reward, self.done, {}  # Возвращаем выбранного жениха если процесс завершен\n",
    "            else:  # Если агент решил отклонить жениха\n",
    "                reward = 0  # Награда 0 за отклонение\n",
    "                self.current_index += 1  # Переходим к следующему жениху\n",
    "                if self.current_index < self.num_grooms:\n",
    "                    return np.array([self.grooms[self.current_index]]), reward, False, {}  # Возвращаем новое состояние\n",
    "                else:\n",
    "                    self.done = True  # Завершаем процесс выбора\n",
    "                    return np.array([self.selected_groom]), reward, self.done, {}  # Возвращаем пустое состояние, так как больше женихов нет\n",
    "        except Exception as e:\n",
    "            return None, 0, True, {\"error\": str(e)}  # Обрабатываем ошибки\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "# Создаем среду\n",
    "env = BrideEnv(num_grooms=10)\n",
    "\n",
    "# Используем алгоритм PPO для обучения модели\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)  # Обучаем модель\n",
    "\n",
    "# Функция для конвертации numpy-объектов в Python-объекты\n",
    "def convert_numpy_to_python(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        return data.tolist()\n",
    "    elif isinstance(data, np.integer):\n",
    "        return int(data)\n",
    "    elif isinstance(data, np.floating):\n",
    "        return float(data)\n",
    "    elif isinstance(data, np.bool_):\n",
    "        return bool(data)\n",
    "    elif isinstance(data, dict):\n",
    "        return {k: convert_numpy_to_python(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [convert_numpy_to_python(i) for i in data]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "# Начинаем процесс выбора лучшего жениха\n",
    "best_groom_found = False\n",
    "iteration = 0\n",
    "log_data = []\n",
    "\n",
    "while not best_groom_found:\n",
    "    iteration += 1\n",
    "    obs = env.reset()  # Сбрасываем состояние среды\n",
    "    done = False\n",
    "    iteration_data = {\n",
    "        \"iteration\": iteration,\n",
    "        \"steps\": []\n",
    "    }\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)  # Модель делает предсказание действия\n",
    "        obs, rewards, done, info = env.step(action)  # Выполняем действие и получаем результат\n",
    "        correct_choice = (env.selected_groom == env.max_groom) if action == 1 else False  # Проверяем правильность выбора\n",
    "        step_data = {\n",
    "            \"observation\": convert_numpy_to_python(obs.tolist()),  # Конвертируем обсервацию\n",
    "            \"action\": int(action),  # Действие агента\n",
    "            \"reward\": convert_numpy_to_python(rewards),  # Награда\n",
    "            \"done\": convert_numpy_to_python(done),  # Завершено ли действие\n",
    "            \"info\": convert_numpy_to_python(info),  # Дополнительная информация\n",
    "            \"current_index\": convert_numpy_to_python(env.current_index),  # Текущий индекс жениха\n",
    "            \"grooms\": convert_numpy_to_python(env.grooms.tolist()),  # Список женихов\n",
    "            \"max_groom\": convert_numpy_to_python(env.max_groom),  # Рейтинг лучшего жениха\n",
    "            \"correct_choice\": convert_numpy_to_python(correct_choice)  # Правильность выбора\n",
    "        }\n",
    "        iteration_data[\"steps\"].append(step_data)\n",
    "        if \"error\" in info:\n",
    "            iteration_data[\"error\"] = info[\"error\"]\n",
    "    log_data.append(iteration_data)\n",
    "    if rewards == 1 and correct_choice:\n",
    "        best_groom_found = True  # Найден лучший жених\n",
    "        print(f\"Обученная модель выбрала лучшего жениха с рейтингом {obs[0]} после {iteration} итераций\")\n",
    "    else:\n",
    "        print(f\"Итерация {iteration}: Модель ошиблась с выбором жениха и направлена на переобучение. Обсервация: {obs}, Рейтинг: {env.grooms}, Награда: {rewards}.\")\n",
    "        model.learn(total_timesteps=10000)  # Дополнительное обучение модели\n",
    "\n",
    "# Сохраняем данные об итерациях в файл\n",
    "try:\n",
    "    with open(\"iteration_log.json\", \"w\") as f:\n",
    "        json.dump(convert_numpy_to_python(log_data), f, ensure_ascii=False, indent=4)\n",
    "        print(\"Данные успешно сохранены в iteration_log.json\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при сохранении данных в файл: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
